# Gemini Context Caching Implementation

## Overview

This implementation adds Gemini's context caching feature to the Communique agent infrastructure, achieving **20-30% overall token cost savings** on repeated requests with stable system prompts and schemas.

## How Gemini's cachedContent API Works

Gemini's context caching API allows you to cache portions of your request (system instructions, schemas, and content) and reference them in subsequent requests at a 90% discount:

1. **Initial Cache Creation**: You send your system instruction and/or schema to Gemini's cache API, which stores it and returns a cache name (`cachedContents/{id}`)
2. **Cache Reference**: On subsequent requests, you reference the cache name instead of sending the full system instruction
3. **Cost Savings**: Cached tokens cost 10% of normal input tokens (90% discount on Gemini 2.5+ models)
4. **TTL Management**: Caches expire after a specified time-to-live (default: 1 hour, max: 24 hours)

### API Flow

```typescript
// Step 1: Create cache
const cache = await ai.caches.create({
  model: 'gemini-3-flash-preview',
  config: {
    systemInstruction: 'You are an expert analyst...',
    responseSchema: { type: 'object', ... },
    ttl: '86400s' // 24 hours
  }
});

// Step 2: Use cache in requests
const response = await ai.models.generateContent({
  model: 'gemini-3-flash-preview',
  contents: 'Your query here',
  config: {
    cachedContent: cache.name // Reference the cached content
  }
});
```

## Architecture

### Cache Manager (`cache-manager.ts`)

The cache manager provides:

1. **Content Hashing**: SHA-256 hashing of system instructions and schemas to generate stable cache keys
2. **Registry Management**: In-memory registry tracking active caches with expiration times
3. **Automatic Cleanup**: Background process removes expired entries every 5 minutes
4. **Cache Lifecycle**: Handles creation, retrieval, invalidation, and expiration

### Integration Points

The caching feature is integrated into three key functions:

1. **`generate()`** - Single-turn generation with optional caching
2. **`generateWithThoughts()`** - Streaming generation with thoughts + caching
3. **`generateStreamWithThoughts()`** - Advanced streaming with caching support

### Cache Key Strategy

Cache keys are generated by hashing:
- System instruction text
- Response schema (JSON stringified with sorted keys for determinism)
- Additional content (if provided)

This ensures:
- **Deterministic**: Same content always produces same hash
- **Collision-resistant**: SHA-256 provides excellent collision resistance
- **Content-based**: Changes to prompt/schema create new cache

Example:
```typescript
const content = {
  systemInstruction: ROLE_DISCOVERY_PROMPT,
  responseSchema: ROLE_DISCOVERY_SCHEMA
};

const hash = hashContent(content);
// → "a3f5d8c9..." (64-character hex string)
```

## TTL and Expiration Handling

### TTL Options

Three predefined TTL levels:

| Level | Duration | Use Case | Example |
|-------|----------|----------|---------|
| `short` | 1 hour (3600s) | Dynamic content that changes frequently | News analysis prompts |
| `medium` | 6 hours (21600s) | Semi-stable content | Daily updated datasets |
| `long` | 24 hours (86400s) | Stable prompts/schemas | Decision-maker discovery prompts |

### Expiration Flow

```
┌─────────────────────────────────────────────────────────────┐
│  Request with enableCaching=true                            │
└───────────────────────┬─────────────────────────────────────┘
                        │
                        ▼
        ┌───────────────────────────────┐
        │  Hash content → cache key     │
        └───────────────┬───────────────┘
                        │
                        ▼
        ┌───────────────────────────────┐
        │  Check local registry         │
        └───────────────┬───────────────┘
                        │
           ┌────────────┴────────────┐
           │                         │
      Found & Valid            Not Found / Expired
           │                         │
           ▼                         ▼
    Return cache name      Create new cache via API
           │                         │
           │                ┌────────┴────────┐
           │                │  Store in registry│
           │                │  with expiration  │
           │                └────────┬────────┘
           │                         │
           └────────────┬────────────┘
                        │
                        ▼
            Use cache name in request
```

### Automatic Cleanup

Background cleanup process runs every 5 minutes:

```typescript
setInterval(() => {
  for (const [hash, entry] of cacheRegistry.entries()) {
    if (entry.expiresAt < now) {
      cacheRegistry.delete(hash); // Remove expired entry
    }
  }
}, 5 * 60 * 1000);
```

## Expected Token Savings Calculation

### Formula

For N requests using cached content of T tokens:

- **Without caching**: `N × T` tokens
- **With caching**: `T + (N-1) × 0.1T` tokens
- **Tokens saved**: `N × T - [T + (N-1) × 0.1T] = 0.9(N-1)T`
- **Percent saved**: `[0.9(N-1)T] / (N × T) = 0.9(N-1) / N`

### Break-Even Analysis

| Requests | Without Caching | With Caching | Tokens Saved | % Saved |
|----------|----------------|--------------|--------------|---------|
| 1 | 1000 | 1000 | 0 | 0% |
| 2 | 2000 | 1100 | 900 | 45% |
| 5 | 5000 | 1400 | 3600 | 72% |
| 10 | 10000 | 1900 | 8100 | 81% |
| 50 | 50000 | 5410 | 44590 | 89.2% |
| 100 | 100000 | 10900 | 89100 | 89.1% |

**Break-even point**: 2 requests (after the initial cache creation, every subsequent request saves 90% of the prompt tokens)

### Real-World Example: Decision-Maker Resolution

The decision-maker agent uses two prompts repeatedly:

1. **Role Discovery Prompt**: ~800 tokens
2. **Person Lookup Prompt**: ~1200 tokens

For a campaign with 100 decision-maker lookups:

**Without caching:**
- Role discovery: 800 × 100 = 80,000 tokens
- Person lookup: 1200 × 100 = 120,000 tokens
- **Total**: 200,000 tokens

**With caching:**
- Role discovery: 800 + (99 × 80) = 8,720 tokens
- Person lookup: 1200 + (99 × 120) = 13,080 tokens
- **Total**: 21,800 tokens

**Savings:**
- Tokens saved: 178,200
- Percent saved: **89.1%**
- Cost reduction: **~10x cheaper**

### Overall Cost Savings

Considering that system prompts typically represent 30-40% of total token usage:

- **Prompt savings**: 89% on 35% of tokens = 31.2% reduction
- **Expected overall savings**: **20-30%** on total API costs

## Usage Examples

### Basic Usage

```typescript
import { generate } from '$lib/core/agents/gemini-client';

const response = await generate('Your query here', {
  systemInstruction: 'You are an expert analyst...',
  responseSchema: MY_SCHEMA,
  enableCaching: true,      // Enable caching
  cacheTTL: 'long',         // 24-hour cache
  cacheDisplayName: 'my-prompt-cache'
});
```

### Decision-Maker Resolution (Production)

```typescript
import { generateWithThoughts } from '$lib/core/agents/gemini-client';
import { ROLE_DISCOVERY_PROMPT } from '$lib/core/agents/prompts/decision-maker';

const roleResult = await generateWithThoughts(
  buildRoleDiscoveryPrompt(subject, message, topics),
  {
    systemInstruction: ROLE_DISCOVERY_PROMPT,
    temperature: 0.3,
    thinkingLevel: 'medium',
    maxOutputTokens: 65536,
    // Enable caching - first request creates cache, rest reuse it
    enableCaching: true,
    cacheTTL: 'long',
    cacheDisplayName: 'role-discovery-prompt'
  }
);
```

### Monitoring Cache Performance

```typescript
import { getCacheStats } from '$lib/core/agents/cache-manager';

const stats = getCacheStats();

console.log('Cache Performance:');
console.log(`  Total caches: ${stats.totalCaches}`);
console.log(`  Valid caches: ${stats.validCaches}`);
console.log(`  Expired caches: ${stats.expiredCaches}`);

stats.cacheDetails.forEach(cache => {
  const timeLeft = Math.round(
    (cache.expiresAt.getTime() - Date.now()) / 1000 / 60
  );
  console.log(`  ${cache.displayName}: ${timeLeft}min remaining`);
});
```

### Cost Estimation

```typescript
import { estimateTokenSavings } from '$lib/core/agents/cache-manager';

const savings = estimateTokenSavings(
  800,  // Role discovery prompt tokens
  100   // Campaign size
);

console.log('Estimated Savings:');
console.log(`  Without caching: ${savings.withoutCaching} tokens`);
console.log(`  With caching: ${savings.withCaching} tokens`);
console.log(`  Tokens saved: ${savings.tokensSaved}`);
console.log(`  Percent saved: ${savings.percentSaved.toFixed(1)}%`);
```

## Best Practices

### When to Enable Caching

✅ **Enable caching for:**
- System prompts that stay constant across requests
- Response schemas that don't change
- Agents that process many similar requests
- Production workloads with predictable patterns

❌ **Don't enable caching for:**
- One-off or exploratory requests
- Highly dynamic prompts that change each time
- Development/testing (adds complexity)
- Requests with unique system instructions

### Cache TTL Selection

- **Short (1h)**: Content that updates frequently (news, dynamic data)
- **Medium (6h)**: Semi-stable content (daily aggregations)
- **Long (24h)**: Stable prompts that rarely change (production agents)

### Cache Naming

Use descriptive names for debugging:

```typescript
cacheDisplayName: 'role-discovery-prompt'     // Good
cacheDisplayName: 'cache-1'                   // Bad
cacheDisplayName: undefined                   // Acceptable (auto-generated)
```

### Production Considerations

1. **Cache Registry**: Current implementation uses in-memory registry
   - For multi-instance deployments, consider Redis or database
   - For single-instance, current approach is sufficient

2. **Monitoring**: Track cache hit rates and savings
   ```typescript
   // Log cache usage periodically
   setInterval(() => {
     const stats = getCacheStats();
     console.log('[cache-metrics]', {
       totalCaches: stats.totalCaches,
       validCaches: stats.validCaches,
       hitRate: calculateHitRate() // Your implementation
     });
   }, 60000);
   ```

3. **Error Handling**: Cache creation failures fall back gracefully
   ```typescript
   // If cache creation fails, request proceeds without caching
   try {
     cachedContentName = await getOrCreateCache(...);
   } catch (error) {
     console.warn('Cache failed, proceeding without:', error);
     // Request continues with full system instruction
   }
   ```

## Testing

Run the test suite:

```bash
npm test tests/unit/agents/cache-manager.test.ts
```

Run examples:

```bash
tsx src/lib/core/agents/cache-manager.example.ts
```

## References

- [Context Caching Overview](https://ai.google.dev/gemini-api/docs/caching) - Official Gemini API documentation
- [Caching API Reference](https://ai.google.dev/api/caching) - API specification
- [Node.js SDK Documentation](https://googleapis.github.io/js-genai/release_docs/classes/caches.Caches.html) - TypeScript SDK reference
- [Context Caching Guide](https://dev.to/rawheel/lowering-your-gemini-api-bill-a-guide-to-context-caching-aag) - Practical guide with examples

## Future Improvements

1. **Distributed Caching**: Redis integration for multi-instance deployments
2. **Cache Analytics**: Track hit rates, savings, and usage patterns
3. **Automatic Cache Warming**: Pre-populate caches for known patterns
4. **Cache Versioning**: Handle prompt updates gracefully
5. **Cost Dashboard**: Real-time visualization of caching savings
